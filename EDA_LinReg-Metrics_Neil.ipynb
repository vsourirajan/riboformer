{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/data_with_human_TE_cellline_all_plain.csv\"\n",
    "df = pd.read_csv(data_path, delimiter=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique cell lines -- number of columns that have \"bio_source\" in the name\n",
    "\n",
    "print(\"Number of columns: \", len(df.columns))\n",
    "print(\"Number of rows: \", len(df))\n",
    "\n",
    "na_rows = df[df.isna().any(axis=1)]\n",
    "print(\"Number of rows that have NA: \", len(na_rows))\n",
    "\n",
    "bio_source_cols = [col for col in df.columns if 'bio_source' in col]\n",
    "print(f\"Number of unique human cell lines: {len(bio_source_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in the null values\n",
    "df = df.dropna()\n",
    "na_rows = df[df.isna().any(axis=1)]\n",
    "print(\"Number of rows that have NA: \", len(na_rows))\n",
    "print(\"Number of rows: \", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_source_cols = [col for col in df.columns if 'bio_source' in col]\n",
    "bio_source_df = df[bio_source_cols]\n",
    "bio_source_df.columns = bio_source_df.columns.str.replace('bio_source_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_corr = bio_source_df.corr(method='spearman')\n",
    "print(\"Dimensions of spearman correlation matrix: \", spearman_corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = 1 - spearman_corr\n",
    "condensed_dist = squareform(distance_matrix.values)\n",
    "linkage_matrix = linkage(condensed_dist, method='average')\n",
    "\n",
    "ordered_indices = leaves_list(linkage_matrix)\n",
    "ordered_corr = spearman_corr.iloc[ordered_indices, ordered_indices]\n",
    "\n",
    "mask = np.triu(np.ones_like(ordered_corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(\n",
    "    ordered_corr, \n",
    "    mask=mask, \n",
    "    cmap='coolwarm', \n",
    "    annot=False, \n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5, \n",
    "    square=True, \n",
    "    cbar_kws={\"shrink\": 0.5}\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.title(\"Spearman Correlation (Hierarchically Clustered)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('spearman_correlation_heatmap.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LinReg model to plot predictions againt observed Mean TE and calculate R^2, Pearson, and Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train linear regression using solely the bio_source\n",
    "# Filter columns for inputs (columns 3:6) and outputs (columns starting with 'bio_source_')\n",
    "input_columns = df.iloc[:, 3:7]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_columns, bio_source_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred.shape)\n",
    "\n",
    "#average the predictions and observations by row\n",
    "y_pred_avg = np.mean(y_pred, axis=1)\n",
    "y_test_avg = np.mean(y_test, axis=1)\n",
    "print(y_pred_avg.shape)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test_avg, y_pred_avg)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Create 2D histogram\n",
    "plt.figure(figsize=(6, 6))\n",
    "hb = plt.hexbin(y_pred_avg, y_test_avg, gridsize=100, cmap='hot', mincnt=1)\n",
    "\n",
    "# Add colorbar\n",
    "cb = plt.colorbar(hb)\n",
    "cb.set_label('Density')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('LinReg prediction (human)')\n",
    "plt.ylabel('Observed Mean TE (human)')\n",
    "plt.title('Density Scatter Plot')\n",
    "\n",
    "# Optional: Set axis limits similar to your image\n",
    "plt.xlim(-1, 2)\n",
    "plt.ylim(-2, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2547182146077034\n",
      "0.5923102212218953\n",
      "0.5048773638065486\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "print(r2_score(y_test_avg, y_pred_avg))\n",
    "print(spearmanr(y_test_avg, y_pred_avg).correlation)\n",
    "print(pearsonr(y_test_avg, y_pred_avg).statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to split a sequence into 3-mers\n",
    "def tokenize_sequence(sequence, k=3):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "# Extract the tx_sequence column\n",
    "sequences = df['tx_sequence']\n",
    "\n",
    "# # Tokenize each sequence into 3-mers\n",
    "tokenized_sequences = sequences.apply(tokenize_sequence)\n",
    "\n",
    "# Create a vocabulary of unique 3-mers and map them to indices\n",
    "unique_kmers = set(kmer for seq in tokenized_sequences for kmer in seq)\n",
    "kmer_to_index = {kmer: idx for idx, kmer in enumerate(unique_kmers)}\n",
    "\n",
    "# Define the embedding layer\n",
    "vocab_size = len(kmer_to_index)\n",
    "embedding_dim = 50  # You can adjust the embedding dimension\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert each sequence of 3-mers into embeddings\n",
    "def sequence_to_embeddings(sequence):\n",
    "    encoded_sequence = [kmer_to_index[kmer] for kmer in sequence]\n",
    "    encoded_sequence_tensor = torch.tensor(encoded_sequence, dtype=torch.long)\n",
    "    return embedding_layer(encoded_sequence_tensor).detach().numpy()\n",
    "\n",
    "# Map each sequence to its embeddings\n",
    "bio_source_df['embeddings'] = tokenized_sequences.apply(sequence_to_embeddings)\n",
    "\n",
    "# Example: Access embeddings for the first sequence\n",
    "print(bio_source_df['embeddings'].iloc[0])\n",
    "\n",
    "# Uses too much memory - Flatten the list of 3-mers and encode them as integers\n",
    "# all_kmers = [kmer for seq in tokenized_sequences for kmer in seq]\n",
    "# label_encoder = LabelEncoder()\n",
    "# encoded_kmers = label_encoder.fit_transform(all_kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from kmer to its embedding \n",
    "# graph embeddings of a given k-mer to visualize similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Create a mapping between k-mers and their embeddings\n",
    "kmer_to_embedding = {kmer: embedding_layer(torch.tensor([idx], dtype=torch.long)).detach().numpy().flatten()\n",
    "                     for kmer, idx in kmer_to_index.items()}\n",
    "\n",
    "# Extract k-mers and their embeddings\n",
    "kmers = list(kmer_to_embedding.keys())\n",
    "embeddings = np.array(list(kmer_to_embedding.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality of embeddings for visualization\n",
    "# You can use PCA or t-SNE\n",
    "def reduce_embeddings(embeddings, method='pca', n_components=2):\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=n_components, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Use 'pca' or 'tsne'.\")\n",
    "    return reducer.fit_transform(embeddings)\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "reduced_embeddings = reduce_embeddings(embeddings[:30], method='tsne', n_components=2)\n",
    "\n",
    "# Plot the reduced embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a mapping from 3-mers to embeddings\n",
    "# vocab_size = len(label_encoder.classes_)\n",
    "# embedding_dim = 50  # You can adjust the embedding dimension\n",
    "# embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# # Convert each sequence of 3-mers into embeddings\n",
    "# def sequence_to_embeddings(sequence):\n",
    "#     encoded_sequence = label_encoder.transform(sequence)\n",
    "#     encoded_sequence_tensor = torch.tensor(encoded_sequence, dtype=torch.long)\n",
    "#     return embedding_layer(encoded_sequence_tensor).detach().numpy()\n",
    "\n",
    "# # Map each sequence to its embeddings\n",
    "# bio_source_df['embeddings'] = tokenized_sequences.apply(sequence_to_embeddings)\n",
    "\n",
    "# # Example: Access embeddings for the first sequence\n",
    "# print(bio_source_df['embeddings'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
