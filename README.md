# riboformer

[Notes](https://docs.google.com/document/d/1EFYa5CW8N_3Y37szdR6jABegmJxG6Zz35qneyN1aUqM/edit?usp=sharing)

### Setup
- create environment (`conda create —name riboformer python=3.10`)
- `conda install pip`
- `conda activate riboformer`
- `pip install -r requirements.txt`
- Set up conda environment with jupyter kernel
    - `conda install -c conda-forge ipykernel`
    - `python -m ipykernel install --user --name=riboformer`
    - install jupyter extension in VS Code

### Project Directory Structure

<pre>
.
├── Basic_transformer_attempt.ipynb # Notebook to explore transformer training on dataset
├── DNABERT_NN.py # DNABERT + NN model and training script
├── DNABERT_frozen.py # DNABERT frozen weights regressor model and training script
├── DNABERT_regression_naive.py # DNABERT + regression model and training script
├── EDA_LinReg-Metrics_Neil.ipynb # Notebook for simple exploratory data analysis and baseline replication
├── EDA_Vaibhav.ipynb # Notebook for simple exploratory data analysis of the dataset
├── README.md # Project overview and documentation
├── baseline_regression.py # Script to train all the baseline regression models on the filtered dataset
├── data
│   ├── CLEANED_data_with_human_TE_cellline_all_plain.csv # Cleaned data from data_with_human_TE_cellline_all_plain.csv
│   ├── data_with_human_TE_cellline_all_NA_plain.csv # Organized data containing empty and null values
│   ├── data_with_human_TE_cellline_all_plain.csv # Organized data with null values cleaned up
│   ├── final_data.csv # Final filtered data for 500-1500 sequence lengths
│   ├── human_TE_cellline_all_NA_plain.csv # Unorganized data containing all human TE data in the dataset
│   └── human_TE_cellline_all_plain.csv # Unorganized data filtering null values
├── dna_transformer.pth # Saved transformer-from-scratch model weights
├── dna_transformer.py # Transformer model from scratch and training script
├── figures # Figures containing the correlation maps of mean TE for all pairwise cell types
│   ├── pearson_correlation_heatmap.png
│   └── spearman_correlation_heatmap.png
├── generate_embeddings_DNABERT.ipynb # Notebook to generate pretrained DNABERT embeddings from filtered dataset
├── requirements.txt # Python package requirements
└── transcript_embeddings.pkl # Pickle file containing pretrained DNABERT embeddings generated by script generate_kmer_embeddings.py
</pre>

### Experiment Information
- We ran all experiments on cpu or on torch mps backend (M series built-in gpu)
    - All experiments run by default on mps backend if it is avaiable and on cpu otherwise
- `python dna_transformer.py` is the only script requiring >10 hours of training while others use pretrained BERT embeddings and complete in <2 hours


### Sample Experiment Run
```
$ source activate riboformer                              # activates the conda environment set up above
$ nohup python dna_transformer.py > out.log 2>&1 &        # runs the script as a nohup command and pushes all output to out.log
```