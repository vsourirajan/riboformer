{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaibhavsourirajan/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
    "inputs = tokenizer(dna, return_tensors = 'pt')[\"input_ids\"]\n",
    "hidden_states = model(inputs)[0] # [1, sequence_length, 768]\n",
    "\n",
    "# embedding with mean pooling\n",
    "embedding_mean = torch.mean(hidden_states[0], dim=0)\n",
    "print(embedding_mean.shape) # expect to be 768\n",
    "\n",
    "# embedding with max pooling\n",
    "embedding_max = torch.max(hidden_states[0], dim=0)[0]\n",
    "print(embedding_max.shape) # expect to be 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SYMBOL</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>tx_size</th>\n",
       "      <th>utr5_size</th>\n",
       "      <th>cds_size</th>\n",
       "      <th>utr3_size</th>\n",
       "      <th>tx_sequence</th>\n",
       "      <th>bio_source_108T</th>\n",
       "      <th>bio_source_12T</th>\n",
       "      <th>...</th>\n",
       "      <th>struct_max_stem_len_UTR5</th>\n",
       "      <th>struct_max_loop_len_UTR5</th>\n",
       "      <th>struct_min_dG_CDS</th>\n",
       "      <th>struct_n_hairpins_CDS</th>\n",
       "      <th>struct_n_bifurc_CDS</th>\n",
       "      <th>struct_n_bulges_CDS</th>\n",
       "      <th>struct_start_stem_CDS</th>\n",
       "      <th>struct_max_stem_len_CDS</th>\n",
       "      <th>struct_max_loop_len_CDS</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAMD11</td>\n",
       "      <td>ENST00000342066.8</td>\n",
       "      <td>ENSG00000187634.12</td>\n",
       "      <td>2557</td>\n",
       "      <td>90</td>\n",
       "      <td>2046</td>\n",
       "      <td>421</td>\n",
       "      <td>GCAGAGCCCAGCAGATCCCTGCGGCGTTCGCGAGGGTGGGACGGGA...</td>\n",
       "      <td>-3.644472</td>\n",
       "      <td>0.986720</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-13.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOC2L</td>\n",
       "      <td>ENST00000327044.7</td>\n",
       "      <td>ENSG00000188976.11</td>\n",
       "      <td>2757</td>\n",
       "      <td>16</td>\n",
       "      <td>2250</td>\n",
       "      <td>491</td>\n",
       "      <td>GCTTCGGGTTGGTGTCATGGCAGCTGCGGGGAGCCGCAAGAGGCGC...</td>\n",
       "      <td>1.060190</td>\n",
       "      <td>0.701399</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-24.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KLHL17</td>\n",
       "      <td>ENST00000338591.8</td>\n",
       "      <td>ENSG00000187961.14</td>\n",
       "      <td>2567</td>\n",
       "      <td>110</td>\n",
       "      <td>1929</td>\n",
       "      <td>528</td>\n",
       "      <td>GGGAGTGAGCGACACAGAGCGGGCCGCCACCGCCGAGCAGCCCTCC...</td>\n",
       "      <td>-1.198005</td>\n",
       "      <td>-1.178952</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HES4</td>\n",
       "      <td>ENST00000304952.11</td>\n",
       "      <td>ENSG00000188290.11</td>\n",
       "      <td>885</td>\n",
       "      <td>124</td>\n",
       "      <td>666</td>\n",
       "      <td>95</td>\n",
       "      <td>GCGGGCCTGGAGCCGGGATCCGCCCTAGGGGCTCGGATCGCCGCGC...</td>\n",
       "      <td>-1.107400</td>\n",
       "      <td>0.158079</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-24.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISG15</td>\n",
       "      <td>ENST00000649529.1</td>\n",
       "      <td>ENSG00000187608.10</td>\n",
       "      <td>637</td>\n",
       "      <td>77</td>\n",
       "      <td>498</td>\n",
       "      <td>62</td>\n",
       "      <td>GGCGGCTGAGAGGCAGCGAACTCATCTTTGCCAGTACAGGAGCTTG...</td>\n",
       "      <td>0.631561</td>\n",
       "      <td>2.013887</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-28.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SYMBOL       transcript_id             gene_id  tx_size  utr5_size  \\\n",
       "0  SAMD11   ENST00000342066.8  ENSG00000187634.12     2557         90   \n",
       "1   NOC2L   ENST00000327044.7  ENSG00000188976.11     2757         16   \n",
       "2  KLHL17   ENST00000338591.8  ENSG00000187961.14     2567        110   \n",
       "3    HES4  ENST00000304952.11  ENSG00000188290.11      885        124   \n",
       "4   ISG15   ENST00000649529.1  ENSG00000187608.10      637         77   \n",
       "\n",
       "   cds_size  utr3_size                                        tx_sequence  \\\n",
       "0      2046        421  GCAGAGCCCAGCAGATCCCTGCGGCGTTCGCGAGGGTGGGACGGGA...   \n",
       "1      2250        491  GCTTCGGGTTGGTGTCATGGCAGCTGCGGGGAGCCGCAAGAGGCGC...   \n",
       "2      1929        528  GGGAGTGAGCGACACAGAGCGGGCCGCCACCGCCGAGCAGCCCTCC...   \n",
       "3       666         95  GCGGGCCTGGAGCCGGGATCCGCCCTAGGGGCTCGGATCGCCGCGC...   \n",
       "4       498         62  GGCGGCTGAGAGGCAGCGAACTCATCTTTGCCAGTACAGGAGCTTG...   \n",
       "\n",
       "   bio_source_108T  bio_source_12T  ...  struct_max_stem_len_UTR5  \\\n",
       "0        -3.644472        0.986720  ...                      10.0   \n",
       "1         1.060190        0.701399  ...                      10.0   \n",
       "2        -1.198005       -1.178952  ...                      10.0   \n",
       "3        -1.107400        0.158079  ...                      16.0   \n",
       "4         0.631561        2.013887  ...                       8.0   \n",
       "\n",
       "   struct_max_loop_len_UTR5  struct_min_dG_CDS  struct_n_hairpins_CDS  \\\n",
       "0                       5.0              -13.9                    2.0   \n",
       "1                       6.0              -24.5                    1.0   \n",
       "2                       4.0              -23.0                    1.0   \n",
       "3                       3.0              -24.8                    1.0   \n",
       "4                       5.0              -28.6                    1.0   \n",
       "\n",
       "   struct_n_bifurc_CDS  struct_n_bulges_CDS  struct_start_stem_CDS  \\\n",
       "0                  1.0                  0.0                    9.0   \n",
       "1                  0.0                  0.0                   21.0   \n",
       "2                  1.0                  1.0                    3.0   \n",
       "3                  0.0                  0.0                    3.0   \n",
       "4                  0.0                  2.0                    2.0   \n",
       "\n",
       "   struct_max_stem_len_CDS  struct_max_loop_len_CDS  fold  \n",
       "0                      4.0                      4.0     4  \n",
       "1                     10.0                      6.0     8  \n",
       "2                      9.0                      5.0     9  \n",
       "3                     11.0                      3.0     7  \n",
       "4                     13.0                      5.0     2  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"./data/data_with_human_TE_cellline_all_plain.csv\"\n",
    "df = pd.read_csv(data_path, delimiter=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns:  102\n",
      "Number of rows:  11153\n",
      "Number of rows that have NA:  354\n",
      "Number of unique human cell lines: 78\n"
     ]
    }
   ],
   "source": [
    "#number of unique cell lines -- number of columns that have \"bio_source\" in the name\n",
    "\n",
    "print(\"Number of columns: \", len(df.columns))\n",
    "print(\"Number of rows: \", len(df))\n",
    "\n",
    "na_rows = df[df.isna().any(axis=1)]\n",
    "print(\"Number of rows that have NA: \", len(na_rows))\n",
    "\n",
    "bio_source_cols = [col for col in df.columns if 'bio_source' in col]\n",
    "print(f\"Number of unique human cell lines: {len(bio_source_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows that have NA:  0\n",
      "Number of rows:  10799\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SYMBOL</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>tx_size</th>\n",
       "      <th>utr5_size</th>\n",
       "      <th>cds_size</th>\n",
       "      <th>utr3_size</th>\n",
       "      <th>tx_sequence</th>\n",
       "      <th>bio_source_108T</th>\n",
       "      <th>bio_source_12T</th>\n",
       "      <th>...</th>\n",
       "      <th>struct_max_stem_len_UTR5</th>\n",
       "      <th>struct_max_loop_len_UTR5</th>\n",
       "      <th>struct_min_dG_CDS</th>\n",
       "      <th>struct_n_hairpins_CDS</th>\n",
       "      <th>struct_n_bifurc_CDS</th>\n",
       "      <th>struct_n_bulges_CDS</th>\n",
       "      <th>struct_start_stem_CDS</th>\n",
       "      <th>struct_max_stem_len_CDS</th>\n",
       "      <th>struct_max_loop_len_CDS</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAMD11</td>\n",
       "      <td>ENST00000342066.8</td>\n",
       "      <td>ENSG00000187634.12</td>\n",
       "      <td>2557</td>\n",
       "      <td>90</td>\n",
       "      <td>2046</td>\n",
       "      <td>421</td>\n",
       "      <td>GCAGAGCCCAGCAGATCCCTGCGGCGTTCGCGAGGGTGGGACGGGA...</td>\n",
       "      <td>-3.644472</td>\n",
       "      <td>0.986720</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-13.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOC2L</td>\n",
       "      <td>ENST00000327044.7</td>\n",
       "      <td>ENSG00000188976.11</td>\n",
       "      <td>2757</td>\n",
       "      <td>16</td>\n",
       "      <td>2250</td>\n",
       "      <td>491</td>\n",
       "      <td>GCTTCGGGTTGGTGTCATGGCAGCTGCGGGGAGCCGCAAGAGGCGC...</td>\n",
       "      <td>1.060190</td>\n",
       "      <td>0.701399</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-24.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KLHL17</td>\n",
       "      <td>ENST00000338591.8</td>\n",
       "      <td>ENSG00000187961.14</td>\n",
       "      <td>2567</td>\n",
       "      <td>110</td>\n",
       "      <td>1929</td>\n",
       "      <td>528</td>\n",
       "      <td>GGGAGTGAGCGACACAGAGCGGGCCGCCACCGCCGAGCAGCCCTCC...</td>\n",
       "      <td>-1.198005</td>\n",
       "      <td>-1.178952</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HES4</td>\n",
       "      <td>ENST00000304952.11</td>\n",
       "      <td>ENSG00000188290.11</td>\n",
       "      <td>885</td>\n",
       "      <td>124</td>\n",
       "      <td>666</td>\n",
       "      <td>95</td>\n",
       "      <td>GCGGGCCTGGAGCCGGGATCCGCCCTAGGGGCTCGGATCGCCGCGC...</td>\n",
       "      <td>-1.107400</td>\n",
       "      <td>0.158079</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-24.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISG15</td>\n",
       "      <td>ENST00000649529.1</td>\n",
       "      <td>ENSG00000187608.10</td>\n",
       "      <td>637</td>\n",
       "      <td>77</td>\n",
       "      <td>498</td>\n",
       "      <td>62</td>\n",
       "      <td>GGCGGCTGAGAGGCAGCGAACTCATCTTTGCCAGTACAGGAGCTTG...</td>\n",
       "      <td>0.631561</td>\n",
       "      <td>2.013887</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-28.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SYMBOL       transcript_id             gene_id  tx_size  utr5_size  \\\n",
       "0  SAMD11   ENST00000342066.8  ENSG00000187634.12     2557         90   \n",
       "1   NOC2L   ENST00000327044.7  ENSG00000188976.11     2757         16   \n",
       "2  KLHL17   ENST00000338591.8  ENSG00000187961.14     2567        110   \n",
       "3    HES4  ENST00000304952.11  ENSG00000188290.11      885        124   \n",
       "4   ISG15   ENST00000649529.1  ENSG00000187608.10      637         77   \n",
       "\n",
       "   cds_size  utr3_size                                        tx_sequence  \\\n",
       "0      2046        421  GCAGAGCCCAGCAGATCCCTGCGGCGTTCGCGAGGGTGGGACGGGA...   \n",
       "1      2250        491  GCTTCGGGTTGGTGTCATGGCAGCTGCGGGGAGCCGCAAGAGGCGC...   \n",
       "2      1929        528  GGGAGTGAGCGACACAGAGCGGGCCGCCACCGCCGAGCAGCCCTCC...   \n",
       "3       666         95  GCGGGCCTGGAGCCGGGATCCGCCCTAGGGGCTCGGATCGCCGCGC...   \n",
       "4       498         62  GGCGGCTGAGAGGCAGCGAACTCATCTTTGCCAGTACAGGAGCTTG...   \n",
       "\n",
       "   bio_source_108T  bio_source_12T  ...  struct_max_stem_len_UTR5  \\\n",
       "0        -3.644472        0.986720  ...                      10.0   \n",
       "1         1.060190        0.701399  ...                      10.0   \n",
       "2        -1.198005       -1.178952  ...                      10.0   \n",
       "3        -1.107400        0.158079  ...                      16.0   \n",
       "4         0.631561        2.013887  ...                       8.0   \n",
       "\n",
       "   struct_max_loop_len_UTR5  struct_min_dG_CDS  struct_n_hairpins_CDS  \\\n",
       "0                       5.0              -13.9                    2.0   \n",
       "1                       6.0              -24.5                    1.0   \n",
       "2                       4.0              -23.0                    1.0   \n",
       "3                       3.0              -24.8                    1.0   \n",
       "4                       5.0              -28.6                    1.0   \n",
       "\n",
       "   struct_n_bifurc_CDS  struct_n_bulges_CDS  struct_start_stem_CDS  \\\n",
       "0                  1.0                  0.0                    9.0   \n",
       "1                  0.0                  0.0                   21.0   \n",
       "2                  1.0                  1.0                    3.0   \n",
       "3                  0.0                  0.0                    3.0   \n",
       "4                  0.0                  2.0                    2.0   \n",
       "\n",
       "   struct_max_stem_len_CDS  struct_max_loop_len_CDS  fold  \n",
       "0                      4.0                      4.0     4  \n",
       "1                     10.0                      6.0     8  \n",
       "2                      9.0                      5.0     9  \n",
       "3                     11.0                      3.0     7  \n",
       "4                     13.0                      5.0     2  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the rows that have null values\n",
    "df = df.dropna()\n",
    "na_rows = df[df.isna().any(axis=1)]\n",
    "print(\"Number of rows that have NA: \", len(na_rows))\n",
    "print(\"Number of rows: \", len(df))\n",
    "df_filtered = df[df['tx_sequence'].apply(len).between(500, 1500)].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/1267 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     23\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(k_mer, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 24\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# [1, sequence_length, 768]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Take the embedding of the first token (CLS token)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m hidden_states[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# [768]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:609\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, output_all_encoded_layers, masked_tokens_mask, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m     first_col_mask[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    607\u001b[0m     subset_mask \u001b[38;5;241m=\u001b[39m masked_tokens_mask \u001b[38;5;241m|\u001b[39m first_col_mask\n\u001b[0;32m--> 609\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m masked_tokens_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:447\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers, subset_mask)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 447\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_all_encoded_layers:\n\u001b[1;32m    455\u001b[0m             all_encoder_layers\u001b[38;5;241m.\u001b[39mappend(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:330\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, cu_seqlens, seqlen, subset_idx, indices, attn_mask, bias)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass for a BERT layer, including both attention and MLP.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    bias: None or (batch, heads, max_seqlen_in_batch, max_seqlen_in_batch)\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(hidden_states, cu_seqlens, seqlen,\n\u001b[1;32m    329\u001b[0m                                   subset_idx, indices, attn_mask, bias)\n\u001b[0;32m--> 330\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:292\u001b[0m, in \u001b[0;36mBertGatedLinearUnitMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    290\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# multiply by the second matrix\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# add the residual connection and post-LN\u001b[39;00m\n\u001b[1;32m    294\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(hidden_states \u001b[38;5;241m+\u001b[39m residual_connection)\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/riboformer/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embedding_means = []\n",
    "embedding_maxs = []\n",
    "transcript_ids = []\n",
    "\n",
    "# Loop over all sequences\n",
    "for idx, row in tqdm(df_filtered.iterrows(), total=len(df_filtered), desc=\"Generating embeddings\"):\n",
    "    transcript_id = row['transcript_id']\n",
    "    dna_sequence = row['tx_sequence']\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "        inputs = tokenizer(dna_sequence, return_tensors='pt')[\"input_ids\"]\n",
    "        hidden_states = model(inputs)[0]  # [1, sequence_length, 768]\n",
    "\n",
    "        # Mean pooling\n",
    "        embedding_mean = torch.mean(hidden_states[0], dim=0)  # [768]\n",
    "        \n",
    "        # Max pooling\n",
    "        embedding_max = torch.max(hidden_states[0], dim=0)[0]  # [768]\n",
    "\n",
    "    # Save to lists\n",
    "    transcript_ids.append(transcript_id)\n",
    "    embedding_means.append(embedding_mean.cpu().numpy())\n",
    "    embedding_maxs.append(embedding_max.cpu().numpy())\n",
    "\n",
    "    print(f\"Transcript generated for {transcript_id}\")\n",
    "\n",
    "\n",
    "# Create a new DataFrame\n",
    "embeddings_df = pd.DataFrame({\n",
    "    'transcript_id': transcript_ids,\n",
    "    'embedding_mean': embedding_means,\n",
    "    'embedding_max': embedding_maxs\n",
    "})\n",
    "\n",
    "# Save to a file (e.g., a CSV or a pickle file)\n",
    "embeddings_df.to_pickle('transcript_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
